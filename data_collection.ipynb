{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.0+cu118\n",
      "2.4.0+cu118\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from torchaudio.pipelines import MMS_FA as bundle\n",
    "from typing import List\n",
    "import IPython\n",
    "import matplotlib.pyplot as plt\n",
    "from pypinyin import pinyin, lazy_pinyin, Style\n",
    "from pydub import AudioSegment\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torchaudio.__version__)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "import whisper\n",
    "from pydub import AudioSegment\n",
    "import librosa\n",
    "from opencc import OpenCC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\Py39\\lib\\site-packages\\whisper\\__init__.py:146: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(fp, map_location=device)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'id': 0,\n",
       "  'seek': 0,\n",
       "  'start': 0.0,\n",
       "  'end': 2.0,\n",
       "  'text': '你好',\n",
       "  'tokens': [50364, 26410, 50464],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.5360870361328125,\n",
       "  'compression_ratio': 0.42857142857142855,\n",
       "  'no_speech_prob': 0.040411192923784256}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "torch.cuda.memory_summary(device=None, abbreviated=False)\n",
    "\n",
    "raw_data = \"raw_audio\\\\record.WAV\"\n",
    "\n",
    "model = whisper.load_model(\"large-v2\")\n",
    "# 使用Whisper進行語音識別\n",
    "result = model.transcribe(raw_data)\n",
    "result['segments']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'|===========================================================================|\\n|                  PyTorch CUDA memory summary, device ID 0                 |\\n|---------------------------------------------------------------------------|\\n|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\\n|===========================================================================|\\n|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\\n|---------------------------------------------------------------------------|\\n| Allocated memory      |   6025 MiB |   8961 MiB |  65967 MiB |  59941 MiB |\\n|       from large pool |   6020 MiB |   8952 MiB |  64336 MiB |  58316 MiB |\\n|       from small pool |      5 MiB |      8 MiB |   1630 MiB |   1624 MiB |\\n|---------------------------------------------------------------------------|\\n| Active memory         |   6025 MiB |   8961 MiB |  65967 MiB |  59941 MiB |\\n|       from large pool |   6020 MiB |   8952 MiB |  64336 MiB |  58316 MiB |\\n|       from small pool |      5 MiB |      8 MiB |   1630 MiB |   1624 MiB |\\n|---------------------------------------------------------------------------|\\n| Requested memory      |   5896 MiB |   8831 MiB |  65635 MiB |  59739 MiB |\\n|       from large pool |   5890 MiB |   8823 MiB |  64006 MiB |  58115 MiB |\\n|       from small pool |      5 MiB |      8 MiB |   1629 MiB |   1623 MiB |\\n|---------------------------------------------------------------------------|\\n| GPU reserved memory   |   6246 MiB |   9718 MiB |   9718 MiB |   3472 MiB |\\n|       from large pool |   6236 MiB |   9708 MiB |   9708 MiB |   3472 MiB |\\n|       from small pool |     10 MiB |     10 MiB |     10 MiB |      0 MiB |\\n|---------------------------------------------------------------------------|\\n| Non-releasable memory | 225591 KiB | 469904 KiB |  29723 MiB |  29503 MiB |\\n|       from large pool | 220806 KiB | 467974 KiB |  28089 MiB |  27873 MiB |\\n|       from small pool |   4785 KiB |   4785 KiB |   1634 MiB |   1629 MiB |\\n|---------------------------------------------------------------------------|\\n| Allocations           |    1263    |    2521    |   16848    |   15585    |\\n|       from large pool |     518    |    1033    |    5257    |    4739    |\\n|       from small pool |     745    |    1488    |   11591    |   10846    |\\n|---------------------------------------------------------------------------|\\n| Active allocs         |    1263    |    2521    |   16848    |   15585    |\\n|       from large pool |     518    |    1033    |    5257    |    4739    |\\n|       from small pool |     745    |    1488    |   11591    |   10846    |\\n|---------------------------------------------------------------------------|\\n| GPU reserved segments |     267    |     444    |     444    |     177    |\\n|       from large pool |     262    |     439    |     439    |     177    |\\n|       from small pool |       5    |       5    |       5    |       0    |\\n|---------------------------------------------------------------------------|\\n| Non-releasable allocs |     138    |     310    |    7932    |    7794    |\\n|       from large pool |     133    |     306    |    3400    |    3267    |\\n|       from small pool |       5    |      16    |    4532    |    4527    |\\n|---------------------------------------------------------------------------|\\n| Oversize allocations  |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| Oversize GPU segments |       0    |       0    |       0    |       0    |\\n|===========================================================================|\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "torch.cuda.memory_summary(device=None, abbreviated=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['你好', 0.0, 2.0]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence =[]\n",
    "cc = OpenCC('s2t')\n",
    "\n",
    "for i in range(len(result['segments'])):\n",
    "    sentence.append([cc.convert(result['segments'][i]['text'].lower().replace('》','').replace('《','').replace('%','').replace('。','').replace('?','').replace('【','').replace('】','').replace('-','').replace('.','').replace(',', '').replace('6','六').replace('4','四').replace('2','二').replace('9','九').replace('8','八').replace('5','五').replace('3','三').replace('0','零').replace('1','一').replace('7','七').replace(' ','').replace('、','')), \n",
    "                     result['segments'][i]['start'], \n",
    "                     result['segments'][i]['end']])\n",
    "   \n",
    "\n",
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio = AudioSegment.from_file(raw_data)\n",
    "for i in range(len(sentence)):\n",
    "    audio_clip = audio[sentence[i][1] *1000: sentence[i][2]*1000]\n",
    "    audio_clip.export(f\"sentences\\\\{(sentence[i][0].lower())}.wav\", format=\"wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = bundle.get_model()\n",
    "model.to(device)\n",
    "\n",
    "tokenizer = bundle.get_tokenizer()\n",
    "aligner = bundle.get_aligner()\n",
    "\n",
    "def compute_alignments(waveform: torch.Tensor, transcript: List[str]):\n",
    "    with torch.inference_mode():\n",
    "        emission, _ = model(waveform.to(device))\n",
    "        token_spans = aligner(emission[0], tokenizer(transcript))\n",
    "    return emission, token_spans\n",
    "\n",
    "def _score(spans):\n",
    "    return sum(s.score * len(s) for s in spans) / sum(len(s) for s in spans)\n",
    "\n",
    "\n",
    "def plot_alignments(waveform, token_spans, emission, transcript, sample_rate=bundle.sample_rate):\n",
    "    ratio = waveform.size(1) / emission.size(1) / sample_rate\n",
    "\n",
    "    fig, axes = plt.subplots(2, 1)\n",
    "    axes[0].imshow(emission[0].detach().cpu().T, aspect=\"auto\")\n",
    "    axes[0].set_title(\"Emission\")\n",
    "    axes[0].set_xticks([])\n",
    "\n",
    "    axes[1].specgram(waveform[0], Fs=sample_rate)\n",
    "    for t_spans, chars in zip(token_spans, transcript):\n",
    "        t0, t1 = t_spans[0].start, t_spans[-1].end\n",
    "        axes[0].axvspan(t0 - 0.5, t1 - 0.5, facecolor=\"None\", hatch=\"/\", edgecolor=\"white\")\n",
    "        axes[1].axvspan(ratio * t0, ratio * t1, facecolor=\"None\", hatch=\"/\", edgecolor=\"white\")\n",
    "        axes[1].annotate(f\"{_score(t_spans):.2f}\", (ratio * t0, sample_rate * 0.51), annotation_clip=False)\n",
    "\n",
    "        for span, char in zip(t_spans, chars):\n",
    "            t0 = span.start * ratio\n",
    "            axes[1].annotate(char, (t0, sample_rate * 0.55), annotation_clip=False)\n",
    "\n",
    "    axes[1].set_xlabel(\"time [second]\")\n",
    "    fig.tight_layout()\n",
    "\n",
    "def preview_word(waveform, spans, num_frames, transcript, sample_rate):\n",
    "    ratio = waveform.size(1) / num_frames\n",
    "    x0 = int(ratio * spans[0].start)\n",
    "    x1 = int(ratio * spans[-1].end)\n",
    "    #print(f\"{transcript} ({_score(spans):.2f}): {x0 / sample_rate:.3f} - {x1 / sample_rate:.3f} sec\")\n",
    "    time_StarAndEnd = [ x0 / sample_rate, x1/sample_rate] # 回傳單個字的起始時間與結束時間\n",
    "    segment = waveform[:, x0:x1]\n",
    "    #return IPython.display.Audio(segment.numpy(), rate=sample_rate)\n",
    "    return time_StarAndEnd\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\Py39\\lib\\site-packages\\torchaudio\\models\\wav2vec2\\components.py:305: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Transcript:  你好\n",
      "Normalized Transcript:  ni hao\n",
      "[['ni3', 0.5131065759637188, 0.6010884353741497], ['hao3', 0.8649886621315193, 1.1435374149659865]]\n",
      "------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,len(sentence)):#len(sentence)-1\n",
    "    text_normalized = ' '.join(lazy_pinyin(sentence[i][0]))#將文字轉為沒有音調的拼音，lazy_pinyin是陣列所以要再join成字串\n",
    "\n",
    "    waveform, sample_rate = librosa.load(f\"sentences\\\\{sentence[i][0]}.wav\")\n",
    "    waveform_tensor = torch.tensor(waveform).unsqueeze(0)\n",
    "\n",
    "    transcript = text_normalized.split()\n",
    "    emission, token_spans = compute_alignments(waveform_tensor, transcript)\n",
    "    num_frames = emission.size(1)\n",
    "\n",
    "\n",
    "    #plot_alignments(waveform, token_spans, emission, transcript)\n",
    "\n",
    "    print(\"Raw Transcript: \", sentence[i][0])\n",
    "    print(\"Normalized Transcript: \", text_normalized)\n",
    "    IPython.display.Audio(waveform, rate=sample_rate)\n",
    "\n",
    "    text_raw = sentence[i][0]\n",
    "    word_start_end = []\n",
    "    pinyin_tone = pinyin(text_raw, style=Style.TONE3, heteronym=False)\n",
    "    for j in range(len(transcript)):#len(transcript)\n",
    "        timeStartEnd = preview_word(waveform_tensor, token_spans[j], num_frames, transcript[j], sample_rate)\n",
    "        word_start_end.append([pinyin_tone[j][0], timeStartEnd[0], timeStartEnd[1]])\n",
    "    print(word_start_end)\n",
    "\n",
    "    audio = AudioSegment.from_file(f\"sentences\\\\{sentence[i][0]}.wav\")\n",
    "    file_name = sentence[i][0]\n",
    "    for k in range(len(word_start_end)):\n",
    "        segment_audio = audio[word_start_end[k][1] *1000: word_start_end[k][2]*1000]\n",
    "        segment_audio.export(f\"data\\\\{file_name}-{k}_{word_start_end[k][0]}.wav\", format=\"wav\")\n",
    "    print('------------------------------------------')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
