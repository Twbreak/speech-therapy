{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.0+cu118\n",
      "2.4.0+cu118\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from torchaudio.pipelines import MMS_FA as bundle\n",
    "from typing import List\n",
    "import IPython\n",
    "import matplotlib.pyplot as plt\n",
    "from pypinyin import pinyin, lazy_pinyin, Style\n",
    "from pydub import AudioSegment\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torchaudio.__version__)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "import whisper\n",
    "from pydub import AudioSegment\n",
    "import librosa\n",
    "from opencc import OpenCC\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import speech_recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'你好'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = speech_recognition.Recognizer()\n",
    "WAV = speech_recognition.AudioFile('raw_audio\\\\record.WAV')\n",
    "with WAV as source:\n",
    "    audio = r.record(source)\n",
    "result = r.recognize_google(audio,language='zh-tw')\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\Py39\\lib\\site-packages\\whisper\\__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(fp, map_location=device)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'id': 0,\n",
       "  'seek': 0,\n",
       "  'start': 0.0,\n",
       "  'end': 2.0,\n",
       "  'text': '你好',\n",
       "  'tokens': [50365, 26410, 50465],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.8126111030578613,\n",
       "  'compression_ratio': 0.42857142857142855,\n",
       "  'no_speech_prob': 3.796431224944996e-11}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "torch.cuda.memory_summary(device=None, abbreviated=False)\n",
    "\n",
    "raw_data = \"raw_audio\\\\record.WAV\"\n",
    "\n",
    "model = whisper.load_model(\"turbo\")\n",
    "# 使用Whisper進行語音識別\n",
    "result = model.transcribe(raw_data)\n",
    "result['segments']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'|===========================================================================|\\n|                  PyTorch CUDA memory summary, device ID 0                 |\\n|---------------------------------------------------------------------------|\\n|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\\n|===========================================================================|\\n|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\\n|---------------------------------------------------------------------------|\\n| Allocated memory      |   3167 MiB |   7871 MiB |  53877 MiB |  50709 MiB |\\n|       from large pool |   3164 MiB |   7863 MiB |  52416 MiB |  49251 MiB |\\n|       from small pool |      3 MiB |      8 MiB |   1461 MiB |   1458 MiB |\\n|---------------------------------------------------------------------------|\\n| Active memory         |   3167 MiB |   7871 MiB |  53877 MiB |  50709 MiB |\\n|       from large pool |   3164 MiB |   7863 MiB |  52416 MiB |  49251 MiB |\\n|       from small pool |      3 MiB |      8 MiB |   1461 MiB |   1458 MiB |\\n|---------------------------------------------------------------------------|\\n| Requested memory      |   3094 MiB |   7723 MiB |  53278 MiB |  50183 MiB |\\n|       from large pool |   3091 MiB |   7715 MiB |  51817 MiB |  48725 MiB |\\n|       from small pool |      3 MiB |      8 MiB |   1460 MiB |   1457 MiB |\\n|---------------------------------------------------------------------------|\\n| GPU reserved memory   |   3290 MiB |   8146 MiB |  14680 MiB |  11390 MiB |\\n|       from large pool |   3280 MiB |   8136 MiB |  14668 MiB |  11388 MiB |\\n|       from small pool |     10 MiB |     10 MiB |     12 MiB |      2 MiB |\\n|---------------------------------------------------------------------------|\\n| Non-releasable memory | 125181 KiB | 331873 KiB |  37583 MiB |  37461 MiB |\\n|       from large pool | 118016 KiB | 329791 KiB |  36116 MiB |  36001 MiB |\\n|       from small pool |   7165 KiB |   7165 KiB |   1466 MiB |   1459 MiB |\\n|---------------------------------------------------------------------------|\\n| Allocations           |     591    |    1768    |   15442    |   14851    |\\n|       from large pool |     238    |     711    |    7399    |    7161    |\\n|       from small pool |     353    |    1057    |    8043    |    7690    |\\n|---------------------------------------------------------------------------|\\n| Active allocs         |     591    |    1768    |   15442    |   14851    |\\n|       from large pool |     238    |     711    |    7399    |    7161    |\\n|       from small pool |     353    |    1057    |    8043    |    7690    |\\n|---------------------------------------------------------------------------|\\n| GPU reserved segments |     136    |     356    |     671    |     535    |\\n|       from large pool |     131    |     351    |     665    |     534    |\\n|       from small pool |       5    |       5    |       6    |       1    |\\n|---------------------------------------------------------------------------|\\n| Non-releasable allocs |      69    |     212    |    6981    |    6912    |\\n|       from large pool |      60    |     204    |    5210    |    5150    |\\n|       from small pool |       9    |      16    |    1771    |    1762    |\\n|---------------------------------------------------------------------------|\\n| Oversize allocations  |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| Oversize GPU segments |       0    |       0    |       0    |       0    |\\n|===========================================================================|\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "torch.cuda.memory_summary(device=None, abbreviated=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['你好', 0.0, 2.0]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence =[]\n",
    "cc = OpenCC('s2t')\n",
    "\n",
    "for i in range(len(result['segments'])):\n",
    "    sentence.append([cc.convert(result['segments'][i]['text'].lower().replace('》','').replace('《','').replace('%','').replace('。','').replace('?','').replace('【','').replace('】','').replace('-','').replace('.','').replace(',', '').replace('6','六').replace('4','四').replace('2','二').replace('9','九').replace('8','八').replace('5','五').replace('3','三').replace('0','零').replace('1','一').replace('7','七').replace(' ','').replace('、','')), \n",
    "                     result['segments'][i]['start'], \n",
    "                     result['segments'][i]['end']])\n",
    "   \n",
    "\n",
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio = AudioSegment.from_file(raw_data)\n",
    "for i in range(len(sentence)):\n",
    "    audio_clip = audio[sentence[i][1] *1000: sentence[i][2]*1000]\n",
    "    audio_clip.export(f\"sentences\\\\{(sentence[i][0].lower())}.wav\", format=\"wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = bundle.get_model()\n",
    "model.to(device)\n",
    "\n",
    "tokenizer = bundle.get_tokenizer()\n",
    "aligner = bundle.get_aligner()\n",
    "\n",
    "def compute_alignments(waveform: torch.Tensor, transcript: List[str]):\n",
    "    with torch.inference_mode():\n",
    "        emission, _ = model(waveform.to(device))\n",
    "        token_spans = aligner(emission[0], tokenizer(transcript))\n",
    "    return emission, token_spans\n",
    "\n",
    "def _score(spans):\n",
    "    return sum(s.score * len(s) for s in spans) / sum(len(s) for s in spans)\n",
    "\n",
    "\n",
    "def plot_alignments(waveform, token_spans, emission, transcript, sample_rate=bundle.sample_rate):\n",
    "    ratio = waveform.size(1) / emission.size(1) / sample_rate\n",
    "\n",
    "    fig, axes = plt.subplots(2, 1)\n",
    "    axes[0].imshow(emission[0].detach().cpu().T, aspect=\"auto\")\n",
    "    axes[0].set_title(\"Emission\")\n",
    "    axes[0].set_xticks([])\n",
    "\n",
    "    axes[1].specgram(waveform[0], Fs=sample_rate)\n",
    "    for t_spans, chars in zip(token_spans, transcript):\n",
    "        t0, t1 = t_spans[0].start, t_spans[-1].end\n",
    "        axes[0].axvspan(t0 - 0.5, t1 - 0.5, facecolor=\"None\", hatch=\"/\", edgecolor=\"white\")\n",
    "        axes[1].axvspan(ratio * t0, ratio * t1, facecolor=\"None\", hatch=\"/\", edgecolor=\"white\")\n",
    "        axes[1].annotate(f\"{_score(t_spans):.2f}\", (ratio * t0, sample_rate * 0.51), annotation_clip=False)\n",
    "\n",
    "        for span, char in zip(t_spans, chars):\n",
    "            t0 = span.start * ratio\n",
    "            axes[1].annotate(char, (t0, sample_rate * 0.55), annotation_clip=False)\n",
    "\n",
    "    axes[1].set_xlabel(\"time [second]\")\n",
    "    fig.tight_layout()\n",
    "\n",
    "def preview_word(waveform, spans, num_frames, transcript, sample_rate):\n",
    "    ratio = waveform.size(1) / num_frames\n",
    "    x0 = int(ratio * spans[0].start)\n",
    "    x1 = int(ratio * spans[-1].end)\n",
    "    #print(f\"{transcript} ({_score(spans):.2f}): {x0 / sample_rate:.3f} - {x1 / sample_rate:.3f} sec\")\n",
    "    time_StarAndEnd = [ x0 / sample_rate, x1/sample_rate] # 回傳單個字的起始時間與結束時間\n",
    "    segment = waveform[:, x0:x1]\n",
    "    #return IPython.display.Audio(segment.numpy(), rate=sample_rate)\n",
    "    return time_StarAndEnd\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\Py39\\lib\\site-packages\\torchaudio\\models\\wav2vec2\\components.py:305: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Transcript:  你好\n",
      "Normalized Transcript:  ni hao\n",
      "[['ni3', 0.5131065759637188, 0.6010884353741497], ['hao3', 0.8649886621315193, 1.1435374149659865]]\n",
      "------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,len(sentence)):#len(sentence)-1\n",
    "    text_normalized = ' '.join(lazy_pinyin(sentence[i][0]))#將文字轉為沒有音調的拼音，lazy_pinyin是陣列所以要再join成字串\n",
    "\n",
    "    waveform, sample_rate = librosa.load(f\"sentences\\\\{sentence[i][0]}.wav\")\n",
    "    waveform_tensor = torch.tensor(waveform).unsqueeze(0)\n",
    "\n",
    "    transcript = text_normalized.split()\n",
    "    emission, token_spans = compute_alignments(waveform_tensor, transcript)\n",
    "    num_frames = emission.size(1)\n",
    "\n",
    "\n",
    "    #plot_alignments(waveform, token_spans, emission, transcript)\n",
    "\n",
    "    print(\"Raw Transcript: \", sentence[i][0])\n",
    "    print(\"Normalized Transcript: \", text_normalized)\n",
    "    IPython.display.Audio(waveform, rate=sample_rate)\n",
    "\n",
    "    text_raw = sentence[i][0]\n",
    "    word_start_end = []\n",
    "    pinyin_tone = pinyin(text_raw, style=Style.TONE3, heteronym=False)\n",
    "    for j in range(len(transcript)):#len(transcript)\n",
    "        timeStartEnd = preview_word(waveform_tensor, token_spans[j], num_frames, transcript[j], sample_rate)\n",
    "        word_start_end.append([pinyin_tone[j][0], timeStartEnd[0], timeStartEnd[1]])\n",
    "    print(word_start_end)\n",
    "\n",
    "    audio = AudioSegment.from_file(f\"sentences\\\\{sentence[i][0]}.wav\")\n",
    "    file_name = sentence[i][0]\n",
    "    for k in range(len(word_start_end)):\n",
    "        segment_audio = audio[word_start_end[k][1] *1000: word_start_end[k][2]*1000]\n",
    "        segment_audio.export(f\"data\\\\{file_name}-{k}_{word_start_end[k][0]}.wav\", format=\"wav\")\n",
    "    print('------------------------------------------')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
